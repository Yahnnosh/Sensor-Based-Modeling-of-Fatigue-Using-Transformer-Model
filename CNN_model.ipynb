{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, Model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, BatchNormalization, Dropout, Input, Concatenate, GlobalAvgPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from evaluator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "VARIABLES = ['ActivityCounts', 'Barometer', 'BloodPerfusion',\n",
    "             'BloodPulseWave', 'EnergyExpenditure', 'GalvanicSkinResponse', 'HR',\n",
    "             'HRV', 'RESP', 'Steps', 'SkinTemperature', 'ActivityClass']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "GRAYSCALE = False # grayscale or rgb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# file path to data folder\n",
    "path = './Output'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dimensions\n",
    "N, HEIGHT, WIDTH, CHANNELS = sum([1 for p in os.listdir(path) if (p[:14] == 'feature_vector' and p[:19] != 'feature_vector_stat')]), \\\n",
    "                             *np.load(path + '/feature_vector0.npy').shape\n",
    "CHANNELS = len(VARIABLES) if GRAYSCALE else CHANNELS # reduce channels for grayscale"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Metadata (subjectID etc.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(path + '/metadata_stat.txt') as f:\n",
    "    metadata = f.read()\n",
    "\n",
    "metadata = json.loads(metadata.replace('\\'', '\\\"').replace('False', 'false').replace('True', 'true')) # doesn't accept other chars"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subjects = [meta['subjectID'] for meta in metadata]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Addditional functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# image-wise transformer\n",
    "def rgb2gray(rgb):\n",
    "    \"\"\"greyscale = 0.2989 * red + 0.5870 * green + 0.1140 * blue\"\"\"\n",
    "    return np.dot(rgb[:, :, :3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "# loss function\n",
    "def weighted_cross_entropy(weight):\n",
    "    def weighted_cross_entropy_with_logits(labels, logits):\n",
    "        loss = tf.nn.weighted_cross_entropy_with_logits(\n",
    "            labels, logits, weight\n",
    "        )\n",
    "        return loss\n",
    "    return weighted_cross_entropy_with_logits\n",
    "\n",
    "# weight (imbalanced classes)\n",
    "def check_imbalance(path_to_labels, indices):\n",
    "    \"\"\"Returns indices of positives/negatives\"\"\"\n",
    "    y = np.empty((len(indices), 2), dtype=int)\n",
    "    for i, index in enumerate(indices):\n",
    "        y[i, ] = np.load(path_to_labels + f'/labels{index}.npy', allow_pickle=True)\n",
    "\n",
    "    positives = np.where(y[:, variable] == 1)[0] # TODO: for now just one variable\n",
    "    negatives = np.where(y[:, variable] == 0)[0] # TODO: for now just one variable\n",
    "\n",
    "    return np.array(indices)[positives], np.array(indices)[negatives]\n",
    "\n",
    "def get_weighting_factor(path, train_set_indices):\n",
    "    positives, negatives = check_imbalance(path, train_set_indices)\n",
    "    sample_weight = len(negatives) / len(positives) # for weighted cross-entropy\n",
    "    return sample_weight"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataloader (dataset with images too large)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, data_path: str, indices_dataset: list, batch_size=32, dim=(HEIGHT, WIDTH), n_channels=CHANNELS, shuffle=True):\n",
    "        self.data_path = data_path # path to full dataset\n",
    "        self.dim = dim # image dimension\n",
    "        self.batch_size = batch_size\n",
    "        self.indices_dataset = indices_dataset # indices of full dataset (different for train/validation/test set)\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.on_epoch_end() # shuffle data for each epoch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Shuffle data for each epoch\n",
    "        \"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices_dataset)\n",
    "\n",
    "    def __data_generation(self, indices):\n",
    "        \"\"\"\n",
    "        Loads and returns datapoints[indices]\n",
    "        \"\"\"\n",
    "        # init\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty(self.batch_size, dtype=float) # TODO: int for non-logits\n",
    "\n",
    "        # load individual datapoints\n",
    "        for i, index in enumerate(indices):\n",
    "            images = np.load(self.data_path + f'/feature_vector{index}.npy', allow_pickle=True)\n",
    "            if GRAYSCALE:\n",
    "                images_gray = np.empty((HEIGHT, WIDTH, self.n_channels))\n",
    "                for j in range(len(VARIABLES)):\n",
    "                    image_rgb = images[:, :, (3 * j): (3 * (j + 1))]\n",
    "                    image_gray = rgb2gray(image_rgb)\n",
    "                    images_gray[:, :, j] = image_gray\n",
    "                images = images_gray\n",
    "\n",
    "            X[i, ] = images\n",
    "            y[i] = np.load(self.data_path + f'/labels{index}.npy', allow_pickle=True)[variable] # TODO: for now just one variable\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of batches per epoch\n",
    "        \"\"\"\n",
    "        return int(np.floor(len(self.indices_dataset) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generates batch[index]\n",
    "        \"\"\"\n",
    "        # calculate indices of batch\n",
    "        indices = self.indices_dataset[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # generate batch\n",
    "        X, y = self.__data_generation(indices)\n",
    "\n",
    "        return X, y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Architecture"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: make possible for grayscale\n",
    "class ConvNet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, name='cnn', **kwargs):\n",
    "        super(CNN, self).__init__(name, **kwargs)\n",
    "\n",
    "        self.in_shape = (HEIGHT, WIDTH, CHANNELS)\n",
    "        self.in_shape_mobilenet = (HEIGHT, WIDTH, 3)\n",
    "\n",
    "        # MobileNetV2 embedding\n",
    "        self.mobilenet = MobileNetV2(input_shape=self.in_shape_mobilenet, weights='imagenet', include_top=False)\n",
    "        self.mobilenet._name = 'mobilenet'\n",
    "        self.mobilenet.trainable = False\n",
    "        self.finetuning = False\n",
    "        self.out_shape_mobilenet = self.mobilenet.layers[-1].output_shape # for one spectrogram\n",
    "\n",
    "        # Concatenation\n",
    "        self.concat = Concatenate(name='concat')\n",
    "\n",
    "        # Global pooling\n",
    "        self.pool = GlobalAvgPool2D(name='global_avg_pool')\n",
    "\n",
    "        # TODO: more sophisticated dense (dropout, regularizer, init., ...)\n",
    "        # Fully-connected network\n",
    "        self.flatten = Flatten(name='flatten', input_shape=(self.out_shape_mobilenet * (CHANNELS // 3), ))\n",
    "        self.dense = Dense(1, name='dense') # keep logits\n",
    "        self.out_shape = 1\n",
    "\n",
    "        # build graph\n",
    "        self.build_graph()\n",
    "\n",
    "    def build_graph(self):\n",
    "        self.build(input_shape=(None, *self.in_shape))\n",
    "        x = Input(shape=self.in_shape)\n",
    "        Model(inputs=[x], outputs=self.call(x))\n",
    "\n",
    "    def set_finetuning(self, mode=True):\n",
    "        self.finetuning = mode\n",
    "        self.mobilenet.trainable = mode\n",
    "\n",
    "        for layers in self.mobilenet.layers:\n",
    "            layers.trainable = False\n",
    "\n",
    "        # \"activate\" last conv layer of MobileNet\n",
    "        self.mobilenet.layers[-3].trainable = mode\n",
    "        self.mobilenet.layers[-2].trainable = mode\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Model predictions (logits)\n",
    "        :param inputs: all spectrograms of shape (HEIGHT, WIDTH, CHANNELS)\n",
    "        :return: class prediction (logits)\n",
    "        \"\"\"\n",
    "        # MobileNetV2 embeddings\n",
    "        x = [self.mobilenet(inputs[..., i:i+3], training=self.finetuning) for i in range(0, CHANNELS, 3)]\n",
    "\n",
    "        # Concatenation\n",
    "        x = self.concat(x)\n",
    "\n",
    "        # Global pooling\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Fully-connected network\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CNN:\n",
    "\n",
    "    def __init__(self, path, variable, epochs, learning_rate, batch_size):\n",
    "        self.model = ConvNet()\n",
    "        self.path = path\n",
    "        assert variable in (0, 1)\n",
    "        self.variable = variable\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.history = None\n",
    "\n",
    "    def fit(self, train_indices):\n",
    "        # training set\n",
    "        train_dataloader = DataGenerator(self.path, train_indices, batch_size=self.batch_size)\n",
    "\n",
    "        # weights for loss function\n",
    "        sample_weights = get_weighting_factor(self.path, train_indices)\n",
    "\n",
    "        # build model\n",
    "        model.compile(optimizer=Adam(learning_rate=self.learning_rate),\n",
    "                      loss=weighted_cross_entropy(sample_weights))\n",
    "\n",
    "        # training\n",
    "        self.history = self.model.fit_generator(generator=train_dataloader,\n",
    "                                                epochs=self.epochs)\n",
    "\n",
    "    def predict(self, test_indices):\n",
    "        \"\"\"Predicts actual class labels (not logits/probability values!)\"\"\"\n",
    "        # TODO: make more efficient\n",
    "        # test set + predict\n",
    "        y_pred = np.empty(len(test_indices), dtype=float)\n",
    "\n",
    "        for i, index in enumerate(test_indices):\n",
    "            X_i = np.load(path + f'/feature_vector{index}.npy', allow_pickle=True)\n",
    "\n",
    "            X_i = tf.expand_dims(X_i, axis=0) # add \"batch dimension\"\n",
    "            logits_pred_i = model.predict(X_i)\n",
    "\n",
    "            y_pred[i] = logits_pred_i\n",
    "\n",
    "        y_probs = tf.math.sigmoid(y_pred) # logits to probs\n",
    "        y_pred = tf.round(y_probs) # probs to labels\n",
    "\n",
    "        return y_pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CV"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    for variable in (0, 1): # phF, MF\n",
    "        model = CNN(path, variable=0, epochs=1, learning_rate=1e-3, batch_size=16)\n",
    "\n",
    "        scores_strat_group_k_fold = stratified_group_k_fold(path=path,\n",
    "                                                            groups=subjects,\n",
    "                                                            model=model,\n",
    "                                                            folds=5,\n",
    "                                                            images=False,\n",
    "                                                            verbose=True,\n",
    "                                                            variable=variable)\n",
    "\n",
    "        scores_strat_k_fold = stratified_k_fold(path=path,\n",
    "                                                groups=subjects,\n",
    "                                                model=model,\n",
    "                                                folds=5,\n",
    "                                                images=False,\n",
    "                                                verbose=True,\n",
    "                                                variable=variable)\n",
    "\n",
    "        scores_loso = leave_one_subject_out(path=path,\n",
    "                                                groups=subjects,\n",
    "                                                model=model,\n",
    "                                                images=False,\n",
    "                                                verbose=True,\n",
    "                                                variable=variable)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
