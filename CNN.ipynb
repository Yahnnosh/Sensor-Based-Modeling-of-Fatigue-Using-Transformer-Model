{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, Model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, BatchNormalization, Dropout, Input, Concatenate, GlobalAvgPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import json\n",
    "from evaluator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "VARIABLES = ['ActivityCounts', 'Barometer', 'BloodPerfusion',\n",
    "             'BloodPulseWave', 'EnergyExpenditure', 'GalvanicSkinResponse', 'HR',\n",
    "             'HRV', 'RESP', 'Steps', 'SkinTemperature', 'ActivityClass']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "GRAYSCALE = False # grayscale or rgb # TODO: currently not working (MobileNetV2 does not support grayscale)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# file path to data folder\n",
    "path = './Output'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613 370 497 30\n"
     ]
    }
   ],
   "source": [
    "# dimensions\n",
    "N, HEIGHT, WIDTH, CHANNELS = sum([1 for p in os.listdir(path) if (p[:14] == 'feature_vector' and p[:19] != 'feature_vector_stat')]), \\\n",
    "                             *np.load(path + '/feature_vector0.npy').shape\n",
    "CHANNELS = len(VARIABLES) if GRAYSCALE else CHANNELS # reduce channels for grayscale\n",
    "\n",
    "print(N, HEIGHT, WIDTH, CHANNELS)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Metadata (subjectID etc.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "with open(path + '/metadata.txt') as f:\n",
    "    metadata = f.read()\n",
    "\n",
    "metadata = json.loads(metadata.replace('\\'', '\\\"').replace('False', 'false').replace('True', 'true')) # doesn't accept other chars"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "subjects = [meta['subjectID'] for meta in metadata]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Addditional functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# image-wise transformer\n",
    "def rgb2gray(rgb):\n",
    "    \"\"\"greyscale = 0.2989 * red + 0.5870 * green + 0.1140 * blue\"\"\"\n",
    "    return np.dot(rgb[:, :, :3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "# loss function\n",
    "def weighted_cross_entropy(weight):\n",
    "    def weighted_cross_entropy_with_logits(labels, logits):\n",
    "        loss = tf.nn.weighted_cross_entropy_with_logits(\n",
    "            labels, logits, weight\n",
    "        )\n",
    "        return loss\n",
    "    return weighted_cross_entropy_with_logits\n",
    "\n",
    "# weight (imbalanced classes)\n",
    "def check_imbalance(path_to_labels, indices, variable):\n",
    "    \"\"\"Returns indices of positives/negatives\"\"\"\n",
    "    y = np.empty((len(indices), 2), dtype=int)\n",
    "    for i, index in enumerate(indices):\n",
    "        y[i, ] = np.load(path_to_labels + f'/labels{index}.npy', allow_pickle=True)\n",
    "\n",
    "    positives = np.where(y[:, variable] == 1)[0]\n",
    "    negatives = np.where(y[:, variable] == 0)[0]\n",
    "\n",
    "    return np.array(indices)[positives], np.array(indices)[negatives]\n",
    "\n",
    "def get_weighting_factor(path, train_set_indices, variable):\n",
    "    positives, negatives = check_imbalance(path, train_set_indices, variable)\n",
    "    sample_weight = len(negatives) / len(positives) # for weighted cross-entropy\n",
    "    return sample_weight"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataloader (dataset with images too large)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, data_path: str, indices_dataset: list, variable, batch_size=32, dim=(HEIGHT, WIDTH), n_channels=CHANNELS, shuffle=True):\n",
    "        self.data_path = data_path # path to full dataset\n",
    "        self.dim = dim # image dimension\n",
    "        self.batch_size = batch_size\n",
    "        self.indices_dataset = indices_dataset # indices of full dataset (different for train/validation/test set)\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        assert variable in (0, 1)\n",
    "        self.variable = variable\n",
    "\n",
    "        self.on_epoch_end() # shuffle data for each epoch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Shuffle data for each epoch\n",
    "        \"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices_dataset)\n",
    "\n",
    "    def __data_generation(self, indices):\n",
    "        \"\"\"\n",
    "        Loads and returns datapoints[indices]\n",
    "        \"\"\"\n",
    "        # init\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty(self.batch_size, dtype=float) # float: logits, int: non-logits\n",
    "\n",
    "        # load individual datapoints\n",
    "        for i, index in enumerate(indices):\n",
    "            images = np.load(self.data_path + f'/feature_vector{index}.npy', allow_pickle=True)\n",
    "            if GRAYSCALE:\n",
    "                images_gray = np.empty((HEIGHT, WIDTH, self.n_channels))\n",
    "                for j in range(len(VARIABLES)):\n",
    "                    image_rgb = images[:, :, (3 * j): (3 * (j + 1))]\n",
    "                    image_gray = rgb2gray(image_rgb)\n",
    "                    images_gray[:, :, j] = image_gray\n",
    "                images = images_gray\n",
    "\n",
    "            X[i, ] = images\n",
    "            y[i] = np.load(self.data_path + f'/labels{index}.npy', allow_pickle=True)[self.variable]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of batches per epoch\n",
    "        \"\"\"\n",
    "        return int(np.floor(len(self.indices_dataset) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generates batch[index]\n",
    "        \"\"\"\n",
    "        # calculate indices of batch\n",
    "        indices = self.indices_dataset[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # generate batch\n",
    "        X, y = self.__data_generation(indices)\n",
    "\n",
    "        return X, y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Architecture"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class ConvNet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, name='cnn', **kwargs):\n",
    "        super(ConvNet, self).__init__(name, **kwargs)\n",
    "\n",
    "        self.in_shape = (HEIGHT, WIDTH, CHANNELS)\n",
    "        self.in_shape_mobilenet = (HEIGHT, WIDTH, 3) if not GRAYSCALE else (HEIGHT, WIDTH, 1)\n",
    "\n",
    "        # MobileNetV2 embedding\n",
    "        self.mobilenet = MobileNetV2(input_shape=self.in_shape_mobilenet, weights='imagenet', include_top=False)\n",
    "        self.mobilenet._name = 'mobilenet'\n",
    "        self.mobilenet.trainable = False\n",
    "        self.finetuning = False\n",
    "        self.out_shape_mobilenet = self.mobilenet.layers[-1].output_shape # for one spectrogram\n",
    "\n",
    "        # Concatenation\n",
    "        self.concat = Concatenate(name='concat')\n",
    "\n",
    "        # Global pooling\n",
    "        self.pool = GlobalAvgPool2D(name='global_avg_pool')\n",
    "\n",
    "        # TODO: more sophisticated dense (dropout, regularizer, init., ...)\n",
    "        # Fully-connected network\n",
    "        self.dense = Dense(1, name='dense') # keep logits\n",
    "        self.out_shape = 1\n",
    "\n",
    "        # build graph\n",
    "        self.build_graph()\n",
    "\n",
    "    def build_graph(self):\n",
    "        self.build(input_shape=(None, *self.in_shape))\n",
    "        x = Input(shape=self.in_shape)\n",
    "        Model(inputs=[x], outputs=self.call(x))\n",
    "\n",
    "    def set_finetuning(self, mode=True):\n",
    "        self.finetuning = mode\n",
    "        self.mobilenet.trainable = mode\n",
    "\n",
    "        for layers in self.mobilenet.layers:\n",
    "            layers.trainable = False\n",
    "\n",
    "        # \"activate\" last conv layer of MobileNet\n",
    "        self.mobilenet.layers[-3].trainable = mode\n",
    "        self.mobilenet.layers[-2].trainable = mode\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Model predictions (logits)\n",
    "        :param inputs: all spectrograms of shape (HEIGHT, WIDTH, CHANNELS)\n",
    "        :return: class prediction (logits)\n",
    "        \"\"\"\n",
    "        # MobileNetV2 embeddings\n",
    "        x = [self.mobilenet(inputs[..., i:i+3], training=self.finetuning) for i in range(0, CHANNELS, 3)]\n",
    "\n",
    "        # Concatenation\n",
    "        x = self.concat(x)\n",
    "\n",
    "        # Global pooling\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Fully-connected network\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class CNN:\n",
    "\n",
    "    def __init__(self, path, variable, epochs, learning_rate, batch_size):\n",
    "        self.model = ConvNet()\n",
    "        self.path = path\n",
    "        assert variable in (0, 1)\n",
    "        self.variable = variable\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.history = None\n",
    "        self.train_indices = None\n",
    "        self.test_indices = None\n",
    "\n",
    "    def fit(self, train_indices):\n",
    "        self.train_indices = train_indices\n",
    "\n",
    "        # training set\n",
    "        train_dataloader = DataGenerator(self.path, train_indices, batch_size=self.batch_size, variable=self.variable)\n",
    "\n",
    "        # weights for loss function\n",
    "        sample_weights = get_weighting_factor(self.path, train_indices, self.variable)\n",
    "\n",
    "        # build model\n",
    "        self.model.compile(optimizer=Adam(learning_rate=self.learning_rate),\n",
    "                           loss=weighted_cross_entropy(sample_weights))\n",
    "\n",
    "        # training\n",
    "        self.history = self.model.fit_generator(generator=train_dataloader,\n",
    "                                                epochs=self.epochs)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets model weights\"\"\"\n",
    "        self.model = ConvNet()\n",
    "\n",
    "    def predict(self, test_indices):\n",
    "        \"\"\"Predicts actual class labels (not logits/probability values!)\"\"\"\n",
    "        self.test_indices = test_indices\n",
    "\n",
    "        # TODO: make more efficient\n",
    "        # test set + predict\n",
    "        y_pred = np.empty(len(test_indices), dtype=float)\n",
    "\n",
    "        for i, index in enumerate(test_indices):\n",
    "            X_i = np.load(path + f'/feature_vector{index}.npy', allow_pickle=True)\n",
    "\n",
    "            X_i = tf.expand_dims(X_i, axis=0) # add \"batch dimension\"\n",
    "            logits_pred_i = self.model.predict(X_i)\n",
    "\n",
    "            y_pred[i] = logits_pred_i\n",
    "\n",
    "        y_probs = tf.math.sigmoid(y_pred) # logits to probs\n",
    "        y_pred = tf.round(y_probs) # probs to labels\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def summary(self):\n",
    "        return self.model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CV"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores_strat_group_k_fold = [None]*2\n",
    "scores_loso = [None]*2\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    for variable in (0, 1): # phF, MF\n",
    "        model = CNN(path, variable=variable, epochs=20, learning_rate=1e-3, batch_size=16)\n",
    "\n",
    "        scores_strat_group_k_fold[variable] = stratified_group_k_fold(path=path,\n",
    "                                                            groups=subjects,\n",
    "                                                            model=model,\n",
    "                                                            folds=5,\n",
    "                                                            images=True,\n",
    "                                                            verbose=True,\n",
    "                                                            variable=variable)\n",
    "\n",
    "        scores_loso[variable] = leave_one_subject_out(path=path,\n",
    "                                            groups=subjects,\n",
    "                                            model=model,\n",
    "                                            images=True,\n",
    "                                            verbose=True,\n",
    "                                            variable=variable)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "path_scores = './Scores'\n",
    "model_name = 'cnn2'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# stratified group 5-fold\n",
    "with open(f'{path_scores}/strat_group_5_fold//{model_name}.txt', 'w') as dat:\n",
    "    dat.write(str(scores_strat_group_k_fold))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# LOSO\n",
    "with open(f'{path_scores}/loso//{model_name}.txt', 'w') as dat:\n",
    "    dat.write(str(scores_loso))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Under development"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Daily majority vote"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''variable = 0\n",
    "model = CNN(path, variable=0, epochs=10, learning_rate=1e-3, batch_size=16)\n",
    "\n",
    "N = sum([1 for p in os.listdir(path) if (p[:14] == 'feature_vector' and p[:19] != 'feature_vector_stat')])\n",
    "\n",
    "# load labels (we need them for stratification)\n",
    "y = np.empty(N, dtype=int)\n",
    "for i in range(N):\n",
    "    y[i] = np.load(path + f'/labels{i}.npy', allow_pickle=True)[variable]  # TODO: multiclass\n",
    "\n",
    "# CV\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "scores_cv = []\n",
    "data_indices = np.arange(N)\n",
    "\n",
    "print(f'Starting stratified group {5}-fold for {[\"physical fatigue\", \"mental fatigue\"][variable]}')\n",
    "with tqdm(total=5) as pbar:\n",
    "    for i, (train_indices, test_indices) in enumerate(cv.split(X=data_indices, y=y, groups=subjects)):\n",
    "        # test labels\n",
    "        y_test = y[test_indices]\n",
    "\n",
    "        # training\n",
    "        model.reset()\n",
    "        model.fit(train_indices)\n",
    "\n",
    "        # predict\n",
    "        y_pred = model.predict(test_indices)\n",
    "\n",
    "        # daily majorities\n",
    "        y_pred_daily = np.array(list(daily_majority_vote(y_pred, test_indices, metadata).values()))\n",
    "        y_true_daily = np.array(list(daily_majority_vote(y_test, test_indices, metadata).values()))\n",
    "\n",
    "        # evaluate\n",
    "        scores = evaluator(y_pred_daily, y_true_daily, verbose=False)\n",
    "        scores_cv.append(scores)\n",
    "\n",
    "        # agreements\n",
    "        print('agreements:', agreements(y_pred, model.test_indices, metadata))\n",
    "\n",
    "        # for progress bar\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f' Fold {i+1} F1: {scores[\"f1\"]}')\n",
    "\n",
    "# print (if verbose==True)\n",
    "print('Performance model:')\n",
    "metrics = scores_cv[0].keys()\n",
    "for metric in metrics:\n",
    "    mean = np.mean([scores_cv_i[metric] for scores_cv_i in scores_cv])\n",
    "    std = np.std([scores_cv_i[metric] for scores_cv_i in scores_cv])\n",
    "    print(f' {metric}: {round(mean, 3)} +- {round(std, 3)} \\n')'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO: check why so few days (even fewer than statistical features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}